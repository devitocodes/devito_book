## Random walk {#sec-diffu-randomwalk}

Models leading to diffusion equations, see Section @sec-diffu-app, are
usually based on reasoning with *averaged* physical quantities such as
concentration, temperature, and velocity.  The underlying physical
processes involve complicated microscopic movement of atoms and
molecules, but an average of a large number of molecules is performed
in a small volume before the modeling starts, and the averaged
quantity inside this volume is assigned as a point value at the
centroid of the volume. This means that concentration, temperature,
and velocity at a space-time point represent averages around the
point in a small time interval and small spatial volume.

Random walk is a principally different kind of modeling procedure
compared to the reasoning behind partial differential equations.  The
idea in random walk is to have a large number of "particles" that
undergo random movements. Averaging can then be used afterwards to
compute macroscopic quantities like concentration. The"particles"
and their random movement represent a very simplified microscopic
behavior of molecules, much simpler and computationally much more
efficient than direct [molecular simulation](https://en.wikipedia.org/wiki/Molecular_dynamics), yet the random
walk model has been very powerful to describe a wide range of
phenomena, including heat conduction, quantum mechanics, polymer
chains, population genetics, neuroscience, hazard games, and pricing
of financial instruments.

It can be shown that random walk, when averaged, produces models that
are mathematically equivalent to diffusion equations. This is the
primary reason why we treat random walk in this chapter: two very
different algorithms (finite difference stencils and random walk)
solve the same type of problems. The simplicity of the random walk
algorithm makes it particularly attractive for solving diffusion
equations on massively parallel computers.
The exposition here is as simple as possible, and good thorough
derivation of the models is provided by Hjorth-Jensen [@hjorten].

## Random walk in 1D {#sec-diffu-randomwalk-1D}

Imagine that we have some particles that perform random moves, either
to the right or to the left. We may flip a coin to decide the movement
of each particle, say head implies movement to the right and tail
means movement to the left. Each move is one unit length.  Physicists
use the term *random walk* for this type of movement.
The movement is also known as [drunkard's walk](https://en.wikipedia.org/wiki/The_Drunkard%27s_Walk).
You may try this yourself: flip the coin and make one step to the left
or right, and repeat the process.

We introduce the symbol $N$ for the number of steps in a random walk.
Figure @fig-diffu-randomwalk-1D-fig-ensemble shows four different
random walks with $N=200$.

![Ensemble of 4 random walks, each with 200 steps.](fig/rw1D_ensemble4){#fig-diffu-randomwalk-1D-fig-ensemble width="100%"}

## Statistical considerations {#sec-diffu-randomwalk-1D-EVar}

Let $S_k$ be the stochastic variable representing a step to the left
or to the right in step number $k$. We have that $S_k=-1$ with
probability $p$ and $S_k=1$ with probability $q=1-p$. The variable
$S_k$ is known as a [Bernoulli variable](https://en.wikipedia.org/wiki/Bernoulli_distribution). The
expectation of $S_k$ is
$$
\E{S_k} = p\cdot (-1) + q\cdot 1 = 1 - 2p,
$$
and the variance is
$$
\Var{S_k} = \E{S_k^2} - \E{S_k}^2 = 1 - (1-2p)^2 = 4p(1-p)\tp
$$
The position after $k$ steps is another stochastic variable
$$
\bar X_k = \sum_{i=0}^{k-1} S_i\tp
$$
The expected position is
$$
\E{\bar X_k} =
\E{\sum_{i=0}^{k-1} S_i} = \sum_{i=0}^{k-1} \E{S_i}= k(1-2p)\tp
$$
All the $S_k$ variables are independent. The variance therefore becomes
$$
\Var{\bar X_k} = \Var{\sum_{i=0}^{k-1} S_i} = \sum_{i=0}^{k-1} \Var{S_i}=
k4p(1-p)\tp
$$
We see that $\Var{\bar X_k}$ is proportional with the number of steps $k$.
For the very important case $p=q=\half$, $\E{\bar X_k}=0$ and
$\Var{\bar X_k}=k$.

How can we estimate $\E{\bar X_k}=0$ and $\Var{\bar X_k}=N$?
We must have many random walks of the type in
Figure @fig-diffu-randomwalk-1D-fig-ensemble. For a given $k$, say $k=100$,
we find all the values of $\bar X_k$, name them $\bar x_{0,k}$, $\bar x_{1,k}$,
$\bar x_{2,k}$, and so on. The empirical estimate of $\E{\bar X_k}$ is the
average,
$$
\E{\bar X_k} \approx \frac{1}{W}\sum_{j=0}^{W-1} \bar x_{j,k},
$$
while an empirical estimate of $\Var{\bar X_k}$ is
$$
\Var{\bar X_k} \approx \frac{1}{W}\sum_{j=0}^{W-1} (\bar x_{j,k})^2 -
\left(\frac{1}{W}\sum_{j=0}^{W-1} \bar x_{j,k}\right)^2\tp
$$
That is, we take the statistics for a given $K$ across the ensemble
of random walks ("vertically" in
Figure @fig-diffu-randomwalk-1D-fig-ensemble). The key quantities
to record are $\sum_i \bar x_{i,k}$ and $\sum_i \bar x_{i,k}^2$.

<!-- Why not horizontally? one can also compute
statistics "horizontally" if....but variance increases with N,
for large periods the path is away from 0... -->

## Playing around with some code

### Scalar code {#sec-diffu-randomwalk-1D-code1}

Python has a `random` module for drawing random numbers, and this
module has a function `uniform(a, b)` for drawing a uniformly
distributed random number in the interval $[a,b)$.  If an event
happens with probability $p$, we can simulate this on the computer by
drawing a random number $r$ in $[0,1)$, because then $r\leq p$ with
probability $p$ and $r>p$ with probability $1-p$:

```python
import random
r = random.uniform(0, 1)
if r <= p:
else:
```

A random walk with $N$ steps, starting at $x_0$, where we move
to the left with probability $p$ and to the right
with probability $1-p$ can now be implemented by

```python
import random, numpy as np

def random_walk1D(x0, N, p):
    """1D random walk with 1 particle."""
    position = np.zeros(N)
    position[0] = x0
    current_pos = x0
    for k in range(N-1):
        r = random.uniform(0, 1)
        if r <= p:
            current_pos -= 1
        else:
            current_pos += 1
        position[k+1] = current_pos
    return position
```

### Vectorized code

Since $N$ is supposed to be large and we want to repeat the process for
many particles, we should speed up the code as much as possible.
Vectorization is the obvious technique here: we draw all the random
numbers at once with aid of `numpy`, and then we formulate vector
operations to get rid of the loop over the steps (`k`).
The `numpy.random` module has vectorized versions of the functions in
Python's built-in `random` module. For example, `numpy.random.uniform(a, b, N)`
returns `N` random numbers uniformly distributed between `a` (included)
and `b` (not included).

We can then make an array of all the steps in a random walk: if
the random number is less than or equal to $p$, the step is $-1$,
otherwise the step is $1$:

```python
r = np.random.uniform(0, 1, size=N)
steps = np.where(r <= p, -1, 1)
```

The value of `position[k]` is the sum of all steps up to step `k`.
Such sums are often needed in vectorized algorithms and therefore
available by the `numpy.cumsum` function:

```python
>>> import numpy as np
>>> np.cumsum(np.array([1,3,4,6]))
array([ 1,  4,  8, 14])
```

The resulting array in this demo has elements $1$, $1+3=4$, $1+3+4=8$,
and $1+3+4+6=14$.

We can now vectorize the `random_walk1D` function:

```python
def random_walk1D_vec(x0, N, p):
    """Vectorized version of random_walk1D."""
    position = np.zeros(N + 1)
    position[0] = x0
    r = np.random.uniform(0, 1, size=N)
    steps = np.where(r <= p, -1, 1)
    position[1:] = x0 + np.cumsum(steps)
    return position
```

This code runs about 10 times faster than the scalar version.
With a parallel `numpy` library, the code can also automatically take
advantage of hardware for parallel computing because each of the four
array operations can be trivially parallelized.

### Fixing the random sequence

During software development with random numbers it is advantageous to
always generate the same sequence of random numbers, as this may help
debugging processes. To fix the sequence, we set a *seed* of the random
number generator to some chosen integer, e.g.,

```python
np.random.seed(10)
```

Calls to `random_walk1D_vec` give positions of the particle as
depicted in Figure @fig-diffu-randomwalk-1D-code1-fig1. The particle starts
at the origin and moves with $p=\half$. Since the seed is the same,
the plot to the left is just a magnification of the first 1,000 steps in
the plot to the right.

![1,000 (left) and 50,000 (right) steps of a random walk.](fig/rw1D_1sample){#fig-diffu-randomwalk-1D-code1-fig1 width="100%"}

### Verification

When we have a scalar and a vectorized code, it is always a good idea to
develop a unit test for checking that they produce the same result.
A problem in the present context is that the two versions apply two different
random number generators. For a test to be meaningful, we need to fix
the seed and use the same generator. This means that the scalar version
must either use `np.random` or have this as an option. An option
is the most flexible choice:

```python
import random

def random_walk1D(x0, N, p, random=random):
    ...
    r = random.uniform(0, 1)
```

Using `random=np.random`, the `r` variable gets computed
by `np.random.uniform`, and the sequence of random numbers will be
the same as in the vectorized version that employs the same generator
(given that the seed is also the same). A proper test function may be
to check that the positions in the walk are the same in the scalar and
vectorized implementations:

```python
def test_random_walk1D():
    x0 = 2
    N = 4
    p = 0.6
    np.random.seed(10)
    scalar_computed = random_walk1D(x0, N, p, random=np.random)
    np.random.seed(10)
    vectorized_computed = random_walk1D_vec(x0, N, p)
    assert (scalar_computed == vectorized_computed).all()
```

Note that we employ `==` for arrays with real numbers, which is normally
an inadequate test due to rounding errors, but in the present case,
all arithmetics consists of adding or subtracting one, so these operations
are expected to have no rounding errors. Comparing two `numpy` arrays
with `==` results in a boolean array, so we need to call the `all()`
method to ensure that all elements are `True`, i.e., that all elements
in the two arrays match each other pairwise.

## Equivalence with diffusion {#sec-diffu-randomwalk-1D-pde}

The original random walk algorithm can be said to work with
dimensionless coordinates $\bar x_i = -N + i$, $i=0,1,\ldots, 2N+1$
($i\in [-N,N]$), and $\bar t_n=n$, $n=0,1,\ldots,N$.  A mesh with
spacings $\Delta x$ and $\Delta t$ with dimensions can be introduced
by
$$
x_i = X_0 + \bar x_i \Delta x,\quad t_n = \bar t_n\Delta t\tp
$$
If we implement the algorithm with dimensionless coordinates, we can just
use this rescaling to obtain the movement in a coordinate system
without unit spacings.

Let $P^{n+1}_i$ be the probability of finding the particle at mesh point
$\bar x_i$ at time $\bar t_{n+1}$. We can reach mesh point $(i,n+1)$ in two
ways: either coming in from the left from $(i-1,n)$ or from the
right ($i+1,n)$. Each has probability $\half$ (if we assume
$p=q=\half$). The fundamental equation for $P^{n+1}_i$ is
$$
P^{n+1}**i = \half P^{n}**{i-1} + \half P^{n}_{i+1}\tp
$$ {#eq-diffu-randomwalk-1D-pde-Markov}
(This equation is easiest to understand if one looks at the random walk
as a Markov process and applies the transition probabilities, but this is
beyond scope of the present text.)

Subtracting $P^{n}_i$ from (@eq-diffu-randomwalk-1D-pde-Markov) results
in
$$
P^{n+1}_i - P^{n}**i = \half (P^{n}**{i-1} -2P^{n}**i + \half P^{n}**{i+1})\tp
$$
Readers who have seen the Forward Euler discretization of a 1D
diffusion equation recognize this scheme as very close to such a
discretization. We have
$$
\frac{\partial}{\partial t}P(x_i,t_{n})
= \frac{P^{n+1}_i - P^{n}_i}{\Delta t} + \Oof{\Delta t},
$$
or in dimensionless coordinates
$$
\frac{\partial}{\partial\bar t}P(\bar x_i,\bar t_n)
\approx P^{n+1}_i - P^{n}_i\tp
$$
Similarly, we have

\begin{align*}
\frac{\partial^2}{\partial x^2}P(x_i,t_n) &=
\frac{P^{n}_{i-1} -2P^{n}**i + \half P^{n}**{i+1}}{\Delta x^2}
+ \Oof{\Delta x^2},\\
\frac{\partial^2}{\partial x^2}P(\bar x_i,\bar t_n) &\approx
P^{n}_{i-1} -2P^{n}**i + \half P^{n}**{i+1}\tp
\end{align*}
Equation (@eq-diffu-randomwalk-1D-pde-Markov) is therefore equivalent with
the dimensionless diffusion equation
$$
\frac{\partial P}{\partial\bar t} = \frac{1}{2}
\frac{\partial^2 P}{\partial \bar x^2},
$$ {#eq-diffu-randomwalk-1D-pde-dimless}
or the diffusion equation
$$
\frac{\partial P}{\partial t} = D\frac{\partial^2 P}{\partial x^2},
$$ {#eq-diffu-randomwalk-1D-pde-dim}
with diffusion coefficient
$$
D = \frac{\Delta x^2}{2\Delta t}\tp
$$
This derivation shows the tight link between random walk and diffusion.
If we keep track of where the particle is, and repeat the process
many times, or run the algorithms for lots of particles, the histogram
of the positions will approximate the solution of the diffusion equation
for the local probability $P^n_i$.

Suppose all the random walks start at the origin. Then the initial
condition for the probability distribution is the Dirac delta
function $\delta(x)$. The solution of (@eq-diffu-randomwalk-1D-pde-dimless)
can be shown to be
$$
\bar P(\bar x,\bar t) = \frac{1}{\sqrt{4\pi\dfc t}}e^{-\frac{x^2}{4\dfc t}},
$$ {#eq-diffu-randomwalk-1D-pde-dimless-sol}
where $\dfc = \half$.

## Implementation of multiple walks
Our next task is to implement an ensemble of walks (for statistics,
see Section @sec-diffu-randomwalk-1D-EVar) and also provide data from
the walks such that we can compute the probabilities of the positions
as introduced in the previous section. An appropriate representation
of probabilities $P^n_i$ are histograms (with $i$ along the $x$ axis)
for a few selected values of $n$.

To estimate the expectation and variance of the random walks,
Section @sec-diffu-randomwalk-1D-EVar points to recording
$\sum_j x_{j,k}$ and $\sum_j x_{j,k}^2$, where $x_{j,k}$ is the
position at time/step level $k$ in random walk number $j$.
The histogram of positions needs the individual values $x_{i,k}$
for all $i$ values and some selected $k$ values.

We introduce `position[k]` to hold $\sum_j x_{j,k}$,
`position2[k]` to hold $\sum_j (x_{j,k})^2$, and
`pos_hist[i,k]` to hold $x_{i,k}$. A selection of $k$ values can be
specified by saying how many, `num_times`, and let them be equally
spaced through time:

```python
pos_hist_times = [(N//num_times)*i for i in range(num_times)]
```
This is one of the few situations where we want integer division (`//`) or
real division rounded to an integer.

### Scalar version
Our scalar implementation of running `num_walks` random walks may go
like this:

```python
import random

import matplotlib.pyplot as plt
import numpy as np

random.seed(10)
np.random.seed(10)

def random_walk1D(x0, N, p, random=random):
    """1D random walk with 1 particle and N moves."""

    position = np.zeros(N + 1)
    position[0] = x0
    current_pos = x0
    for k in range(N):
        r = random.uniform(0, 1)
        if r <= p:
            current_pos -= 1
        else:
            current_pos += 1
        position[k + 1] = current_pos
    return position

def random_walk1D_vec(x0, N, p):
    """Vectorized version of random_walk1D."""
    position = np.zeros(N + 1)
    position[0] = x0
    r = np.random.uniform(0, 1, size=N)
    steps = np.where(r <= p, -1, 1)
    position[1:] = x0 + np.cumsum(steps)
    return position

def test_random_walk1D():
    x0 = 2
    N = 4
    p = 0.6
    np.random.seed(10)
    scalar_computed = random_walk1D(x0, N, p, random=np.random)
    np.random.seed(10)
    vectorized_computed = random_walk1D_vec(x0, N, p)
    assert (scalar_computed == vectorized_computed).all()

def demo_random_walk1D(N=50000):
    np.random.seed(10)
    pos = random_walk1D_vec(x0=0, N=N, p=0.5)
    plt.figure()
    plt.plot(pos)
    plt.savefig("tmp1.pdf")
    plt.savefig("tmp1.png")
    plt.figure()
    plt.plot(pos * pos)
    plt.savefig("tmp2.pdf")
    plt.savefig("tmp2.png")
    plt.show()

def demo_fig_random_walk1D(N=200):
    """Make ensamble of positions (to illustrate E[] operator)."""
    np.random.seed(10)
    num_plots = 4
    for n in range(num_plots):
        plt.subplot(num_plots, 1, n + 1)
        pos = random_walk1D_vec(x0=0, N=N, p=0.5)
        plt.plot(pos)
        plt.axis([0, N, -15, 20])
    plt.savefig("tmp.pdf")
    plt.savefig("tmp.png")
    plt.show()

def demo_random_walk1D_timing():
    import time

    x0 = 0
    N = 10000000
    p = 0.5

    t0 = time.perf_counter()
    np.random.seed(10)
    pos = random_walk1D(x0, N, p, random=np.random)
    t1 = time.perf_counter()
    cpu_scalar = t1 - t0
    print("CPU scalar: %.1f" % cpu_scalar)
    np.random.seed(10)
    pos = random_walk1D_vec(x0, N, p)
    t2 = time.perf_counter()
    cpu_vec = t2 - t1
    print("CPU vectorized: %.1f" % cpu_vec)
    print("CPU scalar/vectorized: %.1f" % (cpu_scalar / cpu_vec))

def random_walks1D(x0, N, p, num_walks=1, num_times=1, random=random):
    """Simulate num_walks random walks from x0 with N steps."""
    position = np.zeros(N + 1)  # Accumulated positions
    position[0] = x0 * num_walks
    position2 = np.zeros(N + 1)  # Accumulated positions**2
    position2[0] = x0**2 * num_walks
    pos_hist = np.zeros((num_walks, num_times))
    pos_hist_times = [(N // num_times) * i for i in range(num_times)]

    for n in range(num_walks):
        num_times_counter = 0
        current_pos = x0
        for k in range(N):
            if k in pos_hist_times:
                pos_hist[n, num_times_counter] = current_pos
                num_times_counter += 1
            r = random.uniform(0, 1)
            if r <= p:
                current_pos -= 1
            else:
                current_pos += 1
            position[k + 1] += current_pos
            position2[k + 1] += current_pos**2
    return position, position2, pos_hist, np.array(pos_hist_times)
```

### Vectorized version
We have already vectorized a single random walk. The additional
challenge here is to vectorize the computation of the data for the
histogram, `pos_hist`, but given the selected steps in `pos_hist_times`,
we can find the corresponding positions by indexing with the
list `pos_hist_times`: `position[post_hist_times]`, which are to be
inserted in `pos_hist[n,:]`.

```python
def random_walks1D_vec1(x0, N, p, num_walks=1, num_times=1):
    """Vectorized version of random_walks1D."""
    position = np.zeros(N + 1)  # Accumulated positions
    position2 = np.zeros(N + 1)  # Accumulated positions**2
    walk = np.zeros(N + 1)  # Positions of current walk
    walk[0] = x0
    pos_hist = np.zeros((num_walks, num_times))
    pos_hist_times = [(N // num_times) * i for i in range(num_times)]

    for n in range(num_walks):
        r = np.random.uniform(0, 1, size=N)
        steps = np.where(r <= p, -1, 1)
        walk[1:] = x0 + np.cumsum(steps)  # Positions of this walk
        position += walk
        position2 += walk**2
        pos_hist[n, :] = walk[pos_hist_times]
    return position, position2, pos_hist, np.array(pos_hist_times)
```

### Improved vectorized version
Looking at the vectorized version above, we still have one potentially
long Python loop over `n`. Normally, `num_walks` will be much larger than `N`.
The vectorization of the loop over `N` certainly speeds up the program,
but if we think of vectorization as also a way to parallelize the code,
all the independent walks (the `n` loop) can be executed in parallel.
Therefore, we should include this loop as well in the vectorized
expressions, at the expense of using more memory.

We introduce the array `walks` to hold the $N+1$ steps of all the walks:
each row represents the steps in one walk.

```python
walks = np.zeros((num_walks, N+1))  # Positions of each walk
walks[:,0] = x0
```
Since all the steps are independent, we can just make one long
vector of enough random numbers (`N*num_walks`), translate these
numbers to $\pm 1$, then we reshape the array such that the steps
of each walk are stored in the rows.

```python
r = np.random.uniform(0, 1, size=N*num_walks)
steps = np.where(r <= p, -1, 1).reshape(num_walks, N)
```

The next step is to sum up the steps in each walk. We need the
`np.cumsum` function for this, with the argument `axis=1` for
indicating a sum across the columns:

```python
>>> a = np.arrange(6).reshape(2,3)
>>> a
array([[0, 1, 2],
       [3, 4, 5]])
>>> np.cumsum(a, axis=1)
array([[ 0,  1,  3],
       [ 3,  7, 12]])
```
Now `walks` can be computed by

```python
walks[:,1:] = x0 + np.cumsum(steps, axis=1)
```

The `position` vector is the sum of all the walks. That is, we want to
sum all the rows, obtained by

```python
position  = np.sum(walks, axis=0)
```
A corresponding expression computes the squares of the positions.
Finally, we need to compute `pos_hist`, but that is a matter of
grabbing some of the walks (according to `pos_hist_times`):

```python
pos_hist[:,:] = walks[:,pos_hist_times]
```
The complete vectorized algorithm without any loop can now be
summarized:

```python
def random_walks1D_vec2(x0, N, p, num_walks=1, num_times=1):
    """Vectorized version of random_walks1D; no loops."""
    position = np.zeros(N + 1)  # Accumulated positions
    position2 = np.zeros(N + 1)  # Accumulated positions**2
    walks = np.zeros((num_walks, N + 1))  # Positions of each walk
    walks[:, 0] = x0
    pos_hist = np.zeros((num_walks, num_times))
    pos_hist_times = [(N // num_times) * i for i in range(num_times)]

    r = np.random.uniform(0, 1, size=N * num_walks)
    steps = np.where(r <= p, -1, 1).reshape(num_walks, N)
    walks[:, 1:] = x0 + np.cumsum(steps, axis=1)
    position = np.sum(walks, axis=0)
    position2 = np.sum(walks**2, axis=0)
    pos_hist[:, :] = walks[:, pos_hist_times]
    return position, position2, pos_hist, np.array(pos_hist_times)
```

What is the gain of the vectorized implementations? One important gain
is that each vectorized operation can be automatically parallelized
if one applies a parallel `numpy` library like [Numba](http://numba.pydata.org). On a single CPU, however, the speed up of the vectorized operations
is also significant. With $N=1,000$ and 50,000 repeated walks,
the two vectorized versions run about 25 and 18 times faster than the scalar
version, with `random_walks1D_vec1` being fastest.

### Remark on vectorized code and parallelization
Our first attempt on vectorization removed the loop over the $N$ steps in
a single walk. However, the number of walks is usually much larger than
$N$, because of the need for accurate statistics. Therefore, we should
rather remove the loop over all walks. It turns out, from our efficiency
experiments, that the function `random_walks1D_vec2` (with no loops) is
slower than `random_walks1D_vec1`. This is a bit surprising and may be
explained by less efficiency in the statements involving very large
arrays, containing all steps for all walks at once.

From a parallelization and improved vectorization point of view, it
would be more natural to switch the sequence of the loops in the
serial code such that the shortest loop is the outer loop:

```python
def random_walks1D2(x0, N, p, num_walks=1, num_times=1, ...):
    ...
    current_pos = x0 + np.zeros(num_walks)
    num_times_counter = -1

    for k in range(N):
        if k in pos_hist_times:
	    num_times_counter += 1
	    store_hist = True
	else:
	    store_hist = False

        for n in range(num_walks):
            r = random.uniform(0, 1)
	    if r <= p:
                current_pos[n] -= 1
            else:
                current_pos[n] += 1
            position [k+1] += current_pos[n]
            position2[k+1] += current_pos[n]**2
            if store_hist:
                pos_hist[n,num_times_counter] = current_pos[n]
    return position, position2, pos_hist, np.array(pos_hist_times)
```
The vectorized version of this code, where we just vectorize the
loop over `n`, becomes

```python
def random_walks1D2_vec1(x0, N, p, num_walks=1, num_times=1):
    """Vectorized version of random_walks1D2."""
    position = np.zeros(N + 1)  # Accumulated positions
    position2 = np.zeros(N + 1)  # Accumulated positions**2
    pos_hist = np.zeros((num_walks, num_times))
    pos_hist_times = [(N // num_times) * i for i in range(num_times)]

    current_pos = np.zeros(num_walks)
    current_pos[0] = x0
    num_times_counter = -1

    for k in range(N):
        if k in pos_hist_times:
            num_times_counter += 1
            store_hist = True  # Store histogram data for this k
        else:
            store_hist = False

        r = np.random.uniform(0, 1, size=num_walks)
        steps = np.where(r <= p, -1, 1)
        current_pos += steps
        position[k + 1] = np.sum(current_pos)
        position2[k + 1] = np.sum(current_pos**2)
        if store_hist:
            pos_hist[:, num_times_counter] = current_pos
    return position, position2, pos_hist, np.array(pos_hist_times)
```
This function runs significantly faster than the `random_walks1D_vec1`
function above, typically 1.7 times faster. The code is also more appropriate
in a parallel computing context since each vectorized statement
can work with data of size `num_walks` over the compute units, repeated `N`
times (compared with data of size `N`, repeated `num_walks` times, in
`random_walks1D_vec1`).

The scalar code with switched loops, `random_walks1D2` runs a bit slower
than the original code in `random_walks1D`, so with the longest loop as
the inner loop, the vectorized function `random_walks1D2_vec1`
is almost 60 times faster than the scalar counterpart, while the
code `random_walks1D_vec2` without loops is only around 18 times faster.
Taking into account the very large arrays required by the latter function,
we end up with `random_walks1D2_vec1` as the preferred implementation.

### Test function
During program development, it is highly recommended to carry out
computations by hand for, e.g., `N=4` and `num_walks=3`.  Normally,
this is done by executing the program with these parameters and
checking with pen and paper that the computations make sense.  The
next step is to use this test for correctness in a formal test
function.

First, we need to check that the simulation of multiple random walks
reproduces the results of `random_walk1D`, `random_walk1D_vec1`, and
`random_walk1D_vec2` for the first walk, if the seed is the
same. Second, we run three random walks (`N=4`) with the scalar and
the two vectorized versions and check that the returned arrays are
identical.

For this type of test to be successful, we must be sure that exactly
the same set of random numbers are used in the three versions, a fact
that requires the same random number generator and the same seed, of
course, but also the same sequence of computations.  This is not
obviously the case with the three `random_walk1D*` functions we
have presented. The critical issue in `random_walk1D_vec1` is that the
first random numbers are used for the first walk, the second set of
random numbers is used for the second walk and so on, to be compatible
with how the random numbers are used in the function `random_walk1D`.
For the function `random_walk1D_vec2` the situation is a bit more
complicated since we generate all the random numbers at once.
However, the critical step now is the reshaping of the array returned
from `np.where`: we must reshape as `(num_walks, N)` to ensure that
the first `N` random numbers are used for the first walk, the next `N`
numbers are used for the second walk, and so on.

We arrive at the test function below.

```python
def test_random_walks1D():
    x0 = 0
    N = 4
    p = 0.5

    num_walks = 1
    np.random.seed(10)
    computed = random_walks1D(x0, N, p, num_walks, random=np.random)
    np.random.seed(10)
    expected = random_walk1D(x0, N, p, random=np.random)
    assert (computed[0] == expected).all()

    np.random.seed(10)
    computed = random_walks1D_vec1(x0, N, p, num_walks)
    np.random.seed(10)
    expected = random_walk1D_vec(x0, N, p)
    assert (computed[0] == expected).all()
    np.random.seed(10)
    computed = random_walks1D_vec2(x0, N, p, num_walks)
    np.random.seed(10)
    expected = random_walk1D_vec(x0, N, p)
    assert (computed[0] == expected).all()

    num_walks = 3
    num_times = N
    np.random.seed(10)
    serial_computed = random_walks1D(x0, N, p, num_walks, num_times, random=np.random)
    np.random.seed(10)
    vectorized1_computed = random_walks1D_vec1(x0, N, p, num_walks, num_times)
    np.random.seed(10)
    vectorized2_computed = random_walks1D_vec2(x0, N, p, num_walks, num_times)
    return_values = ["pos", "pos2", "pos_hist", "pos_hist_times"]
    for s, v, r in zip(serial_computed, vectorized1_computed, return_values):
        msg = "%s: %s (serial) vs %s (vectorized)" % (r, s, v)
        assert (s == v).all(), msg
    for s, v, r in zip(serial_computed, vectorized2_computed, return_values):
        msg = "%s: %s (serial) vs %s (vectorized)" % (r, s, v)
        assert (s == v).all(), msg
```
Such test functions are indispensable for further development of the code
as we can at any time test whether the basic computations remain correct or not.
This is particularly important in stochastic simulations since without
test functions and fixed seeds, we always experience variations from run to
run, and it can be very difficult to spot bugs through averaged statistical
quantities.

## Demonstration of multiple walks
Assuming now that the code works, we can just scale up the number of
steps in each walk and the number of walks. The latter influences the
accuracy of the statistical estimates.  Figure
@fig-diffu-randomwalk-1D-fig-demo1-EX shows the impact of the number
of walks on the expectation, which should approach zero.  Figure
@fig-diffu-randomwalk-1D-fig-demo1-VarX displays the corresponding
estimate of the variance of the position, which should grow linearly
with the number of steps. It does, seemingly very accurately, but
notice that the scale on the $y$ axis is so much larger than for the
expectation, so irregularities due to the stochastic nature of the
process become so much less visible in the variance plots.  The
probability of finding a particle at a certain position at time (or
step) 800 is shown in Figure
@fig-diffu-randomwalk-1D-fig-demo1-HistX. The dashed red line is the
theoretical distribution (@eq-diffu-randomwalk-1D-pde-dimless-sol)
arising from solving the diffusion equation
(@eq-diffu-randomwalk-1D-pde-dimless) instead. As always, we realize
that one needs significantly more statistical samples to estimate a
histogram accurately than the expectation or variance.

![Estimated expected value for 1000 steps, using 100 walks (upper left), 10,000 (upper right), 100,000 (lower left), and 1,000,000 (lower right).](fig/rw1D_EX_100_10000_100000_1000000){#fig-diffu-randomwalk-1D-fig-demo1-EX width="100%"}

![Estimated variance over 1000 steps, using 100 walks (upper left), 10,000 (upper right), 100,000 (lower left), and 1,000,000 (lower right).](fig/rw1D_VarX_100_10000_100000_1000000){#fig-diffu-randomwalk-1D-fig-demo1-VarX width="100%"}

![Estimated probability distribution at step 800, using 100 walks (upper left), 10,000 (upper right), 100,000 (lower left), and 1,000,000 (lower right).](fig/rw1D_HistX_100_10000_100000_1000000){#fig-diffu-randomwalk-1D-fig-demo1-HistX width="100%"}

## Empty figure cache
\clearpage

## Random walk as a stochastic equation {#sec-diffu-randomwalk-1D-ode}

The (dimensionless) position in a random walk, $\bar X_k$, can be expressed as
a stochastic difference equation:
$$
\bar X_k = \bar X_{k-1} + s, \quad x_0=0,
$$ {#eq-diffu-randomwalk-1D-ode-x}
where $s$ is a [Bernoulli variable](https://en.wikipedia.org/wiki/Bernoulli_distribution),
taking on the two values $s=-1$ and $s=1$
with equal probability:
$$
\hbox{P}(s=1)=\half,\quad \hbox{P}(s=-1)=\half\tp
$$
The $s$ variable in a step is independent of the $s$ variable in other steps.

The difference equation expresses essentially the sum of independent
Bernoulli variables.  Because of the central limit theorem, $X_k$,
will then be normally distributed with expectation $k\E{s}$ and
$k\Var{s}$.  The expectation and variance of a Bernoulli variable with
values $r=0$ and $r=1$ are $p$ and $p(1-p)$, respectively.
The variable $s=2r-1$ then has expectation
$2\E{r}-1=2p-1=0$ and variance $2^2\Var{r}=4p(1-p)=1$.  The position
$X_k$ is normally distributed with zero expectation and variance $k$,
as we found in Section @sec-diffu-randomwalk-1D-EVar.

The central limit theorem tells that as long as $k$ is not small,
the distribution of $X_k$ remains the same if
we replace the Bernoulli variable $s$ by any other stochastic variable with
the same expectation and variance. In particular, we may let $s$ be a
standardized Gaussian variable (zero mean, unit variance).

Dividing (@eq-diffu-randomwalk-1D-ode-x) by $\Delta t$ gives
$$
\frac{\bar X_k - \bar X_{k-1}}{\Delta t} = \frac{1}{\Delta t} s\tp
$$
In the limit $\Delta t\rightarrow 0$, $s/\Delta t$ approaches a white noise
stochastic process.
With $\bar X(t)$ as the continuous process in the limit
$\Delta t\rightarrow 0$ ($X_k\rightarrow X(t_k)$),
we formally get the stochastic differential equation
$$
d\bar X = dW,
$$
where $W(t)$ is a [Wiener process](https://en.wikipedia.org/wiki/Wiener_process). Then $X$ is also a
Wiener process. It follows from the stochastic ODE $dX=dW$ that the
probability distribution of $X$ is given by the [Fokker-Planck
equation](https://en.wikipedia.org/wiki/Fokker-Planck_equation)
(@eq-diffu-randomwalk-1D-pde-dimless).  In other words, the key
results for random walk we found earlier can alternatively be
derived via a stochastic ordinary differential equation and its
related Fokker-Planck equation.

## Random walk in 2D
The most obvious generalization of 1D random walk to two spatial
dimensions is to allow movements to the north, east, south, and west,
with equal probability $\frac{1}{4}$.

```python
def random_walk2D(x0, N, p, random=random):
    """2D random walk with 1 particle and N moves: N, E, W, S."""
    d = len(x0)
    position = np.zeros((N + 1, d))
    position[0, :] = x0
    current_pos = np.array(x0, dtype=float)
    for k in range(N):
        r = random.uniform(0, 1)
        if r <= 0.25:
            current_pos += np.array([0, 1])  # Move north
        elif 0.25 < r <= 0.5:
            current_pos += np.array([1, 0])  # Move east
        elif 0.5 < r <= 0.75:
            current_pos += np.array([0, -1])  # Move south
        else:
            current_pos += np.array([-1, 0])  # Move west
        position[k + 1, :] = current_pos
    return position
```
The left plot in Figure @fig-diffu-randomwalk-2D-fig-rect-vs-diag provides
an example on 200 steps with this kind of walk. We may refer to this walk
as a walk on a *rectangular mesh* as we move from any spatial
mesh point $(i,j)$ to one of its four neighbors in the rectangular directions:
$(i+1,j)$, $(i-1,j)$, $(i,j+1)$, or $(i,j-1)$.

![Random walks in 2D with 200 steps: rectangular mesh (left) and diagonal mesh (right).](fig/rw2D_sample200){#fig-diffu-randomwalk-2D-fig-rect-vs-diag width="100%"}

## Random walk in any number of space dimensions
From a programming point of view, especially when implementing a random
walk in any number of dimensions, it is more natural to consider a walk
in the diagonal directions NW, NE, SW, and SE. On a two-dimensional spatial mesh
it means that we go from $(i,j)$ to either $(i+1,j+1)$, $(i-1,j+1)$,
$(i+1,j-1)$, or $(i-1,j-1)$. We can with such a *diagonal mesh*
(see right plot in Figure @fig-diffu-randomwalk-2D-fig-rect-vs-diag)
draw a Bernoulli variable for the step in each spatial direction and
trivially write code that works in any number of spatial directions:

```python
import random

import matplotlib.pyplot as plt
import numpy as np

random.seed(10)
np.random.seed(10)

def random_walk1D(x0, N, p, random=random):
    """1D random walk with 1 particle and N moves."""

    position = np.zeros(N + 1)
    position[0] = x0
    current_pos = x0
    for k in range(N):
        r = random.uniform(0, 1)
        if r <= p:
            current_pos -= 1
        else:
            current_pos += 1
        position[k + 1] = current_pos
    return position

def random_walk1D_vec(x0, N, p):
    """Vectorized version of random_walk1D."""
    position = np.zeros(N + 1)
    position[0] = x0
    r = np.random.uniform(0, 1, size=N)
    steps = np.where(r <= p, -1, 1)
    position[1:] = x0 + np.cumsum(steps)
    return position

def test_random_walk1D():
    x0 = 2
    N = 4
    p = 0.6
    np.random.seed(10)
    scalar_computed = random_walk1D(x0, N, p, random=np.random)
    np.random.seed(10)
    vectorized_computed = random_walk1D_vec(x0, N, p)
    assert (scalar_computed == vectorized_computed).all()

def demo_random_walk1D(N=50000):
    np.random.seed(10)
    pos = random_walk1D_vec(x0=0, N=N, p=0.5)
    plt.figure()
    plt.plot(pos)
    plt.savefig("tmp1.pdf")
    plt.savefig("tmp1.png")
    plt.figure()
    plt.plot(pos * pos)
    plt.savefig("tmp2.pdf")
    plt.savefig("tmp2.png")
    plt.show()

def demo_fig_random_walk1D(N=200):
    """Make ensamble of positions (to illustrate E[] operator)."""
    np.random.seed(10)
    num_plots = 4
    for n in range(num_plots):
        plt.subplot(num_plots, 1, n + 1)
        pos = random_walk1D_vec(x0=0, N=N, p=0.5)
        plt.plot(pos)
        plt.axis([0, N, -15, 20])
    plt.savefig("tmp.pdf")
    plt.savefig("tmp.png")
    plt.show()

def demo_random_walk1D_timing():
    import time

    x0 = 0
    N = 10000000
    p = 0.5

    t0 = time.perf_counter()
    np.random.seed(10)
    pos = random_walk1D(x0, N, p, random=np.random)
    t1 = time.perf_counter()
    cpu_scalar = t1 - t0
    print("CPU scalar: %.1f" % cpu_scalar)
    np.random.seed(10)
    pos = random_walk1D_vec(x0, N, p)
    t2 = time.perf_counter()
    cpu_vec = t2 - t1
    print("CPU vectorized: %.1f" % cpu_vec)
    print("CPU scalar/vectorized: %.1f" % (cpu_scalar / cpu_vec))

def random_walks1D(x0, N, p, num_walks=1, num_times=1, random=random):
    """Simulate num_walks random walks from x0 with N steps."""
    position = np.zeros(N + 1)  # Accumulated positions
    position[0] = x0 * num_walks
    position2 = np.zeros(N + 1)  # Accumulated positions**2
    position2[0] = x0**2 * num_walks
    pos_hist = np.zeros((num_walks, num_times))
    pos_hist_times = [(N // num_times) * i for i in range(num_times)]

    for n in range(num_walks):
        num_times_counter = 0
        current_pos = x0
        for k in range(N):
            if k in pos_hist_times:
                pos_hist[n, num_times_counter] = current_pos
                num_times_counter += 1
            r = random.uniform(0, 1)
            if r <= p:
                current_pos -= 1
            else:
                current_pos += 1
            position[k + 1] += current_pos
            position2[k + 1] += current_pos**2
    return position, position2, pos_hist, np.array(pos_hist_times)

def random_walks1D_vec1(x0, N, p, num_walks=1, num_times=1):
    """Vectorized version of random_walks1D."""
    position = np.zeros(N + 1)  # Accumulated positions
    position2 = np.zeros(N + 1)  # Accumulated positions**2
    walk = np.zeros(N + 1)  # Positions of current walk
    walk[0] = x0
    pos_hist = np.zeros((num_walks, num_times))
    pos_hist_times = [(N // num_times) * i for i in range(num_times)]

    for n in range(num_walks):
        r = np.random.uniform(0, 1, size=N)
        steps = np.where(r <= p, -1, 1)
        walk[1:] = x0 + np.cumsum(steps)  # Positions of this walk
        position += walk
        position2 += walk**2
        pos_hist[n, :] = walk[pos_hist_times]
    return position, position2, pos_hist, np.array(pos_hist_times)

def random_walks1D_vec2(x0, N, p, num_walks=1, num_times=1):
    """Vectorized version of random_walks1D; no loops."""
    position = np.zeros(N + 1)  # Accumulated positions
    position2 = np.zeros(N + 1)  # Accumulated positions**2
    walks = np.zeros((num_walks, N + 1))  # Positions of each walk
    walks[:, 0] = x0
    pos_hist = np.zeros((num_walks, num_times))
    pos_hist_times = [(N // num_times) * i for i in range(num_times)]

    r = np.random.uniform(0, 1, size=N * num_walks)
    steps = np.where(r <= p, -1, 1).reshape(num_walks, N)
    walks[:, 1:] = x0 + np.cumsum(steps, axis=1)
    position = np.sum(walks, axis=0)
    position2 = np.sum(walks**2, axis=0)
    pos_hist[:, :] = walks[:, pos_hist_times]
    return position, position2, pos_hist, np.array(pos_hist_times)

def test_random_walks1D():
    x0 = 0
    N = 4
    p = 0.5

    num_walks = 1
    np.random.seed(10)
    computed = random_walks1D(x0, N, p, num_walks, random=np.random)
    np.random.seed(10)
    expected = random_walk1D(x0, N, p, random=np.random)
    assert (computed[0] == expected).all()

    np.random.seed(10)
    computed = random_walks1D_vec1(x0, N, p, num_walks)
    np.random.seed(10)
    expected = random_walk1D_vec(x0, N, p)
    assert (computed[0] == expected).all()
    np.random.seed(10)
    computed = random_walks1D_vec2(x0, N, p, num_walks)
    np.random.seed(10)
    expected = random_walk1D_vec(x0, N, p)
    assert (computed[0] == expected).all()

    num_walks = 3
    num_times = N
    np.random.seed(10)
    serial_computed = random_walks1D(x0, N, p, num_walks, num_times, random=np.random)
    np.random.seed(10)
    vectorized1_computed = random_walks1D_vec1(x0, N, p, num_walks, num_times)
    np.random.seed(10)
    vectorized2_computed = random_walks1D_vec2(x0, N, p, num_walks, num_times)
    return_values = ["pos", "pos2", "pos_hist", "pos_hist_times"]
    for s, v, r in zip(serial_computed, vectorized1_computed, return_values):
        msg = "%s: %s (serial) vs %s (vectorized)" % (r, s, v)
        assert (s == v).all(), msg
    for s, v, r in zip(serial_computed, vectorized2_computed, return_values):
        msg = "%s: %s (serial) vs %s (vectorized)" % (r, s, v)
        assert (s == v).all(), msg

def demo_random_walks1D(N=1000, num_walks=10000, EX_minmax=None):
    import time

    t0 = time.perf_counter()
    pos, pos2, hist, hist_times = random_walks1D_vec1(
        x0=0,
        N=N,
        p=0.5,
        num_walks=num_walks,
        num_times=10,
    )
    t1 = time.perf_counter()
    print("histogram times:", hist_times)
    print("random walk: %.1fs" % (t1 - t0))
    E_X = pos / float(num_walks)
    Var_X = pos2 / float(num_walks) - E_X**2
    if N <= 50:
        print(pos)

    plt.figure()
    plt.plot(E_X)
    if EX_minmax is not None:
        plt.axis([0, N, EX_minmax[0], EX_minmax[1]])
    plt.title("Expected position (%d walks)" % num_walks)
    plt.savefig("tmp1.png")
    plt.savefig("tmp1.pdf")
    plt.figure()
    plt.plot(Var_X)
    plt.title("Variance of position (%d walks)" % num_walks)
    plt.savefig("tmp2.png")
    plt.savefig("tmp2.pdf")

    plt.figure()
    a = 0.5
    exact = (
        lambda x, t: 1.0 / np.sqrt(4 * np.pi * t * a) * np.exp(-(x**2) / (4.0 * t * a))
    )
    hist_time_index = -2
    n, bins, patches = plt.hist(hist[:, hist_time_index], bins=30, normed=True)
    x = np.linspace(bins[0], bins[-1], 301)
    t = hist_times[hist_time_index]
    plt.plot(x, exact(x, t), "r--")
    plt.title("Histogram of positions (%d walks)" % num_walks)
    plt.savefig("tmp3.png")
    plt.savefig("tmp3.pdf")
    plt.show()

def demo_fig_random_walks1D():
    """Make figures with statistics and dependence on no of walks."""
    import os
    import shutil

    N = 1000
    num_walks = [100, 10000, 100000, 1000000]
    for n in num_walks:
        np.random.seed(10)  # Use same seq. for all experiments
        if n == 100:
            demo_random_walks1D(N=N, num_walks=n, EX_minmax=None)
        else:
            demo_random_walks1D(N=N, num_walks=n, EX_minmax=[-0.1, 0.5])
        d = "tmp_%d" % n
        if os.path.isdir(d):
            shutil.rmtree(d)
        os.mkdir(d)
        for p in 1, 2, 3:
            os.rename("tmp%d.png" % p, os.path.join(d, "tmp%d.png" % p))
            os.rename("tmp%d.pdf" % p, os.path.join(d, "tmp%d.pdf" % p))
    plots = ["EX", "VarX", "HistX"]
    for j, plot in enumerate(plots):
        for ext in "png", "pdf":
            files = [
                os.path.join("tmp_%d" % n, "tmp%d.%s" % (j + 1, ext)) for n in num_walks
            ]
            cmd = "doconce combine_images -%d %s rw1D_%s_%s.%s" % (
                3 if len(num_walks) == 3 else 2,
                " ".join(files),
                plot,
                "_".join([str(n) for n in num_walks]),
                ext,
            )
            print(cmd)
            os.system(cmd)

def demo_random_walks1D_timing():
    import time

    x0 = 0
    N = 1000
    num_walks = 50000
    p = 0.5

    t0 = time.perf_counter()
    np.random.seed(10)
    pos, pos2, pos_hist, pos_hist_times = random_walks1D(
        x0, N, p, num_walks, num_times=4, random=np.random
    )
    t1 = time.perf_counter()
    cpu_scalar = t1 - t0
    print("CPU scalar: %.1f" % cpu_scalar)
    np.random.seed(10)
    pos, pos2, pos_hist, pos_hist_times = random_walks1D_vec1(
        x0, N, p, num_walks, num_times=4
    )
    t2 = time.perf_counter()
    cpu_vec1 = t2 - t1
    print("CPU vectorized1: %.1f" % cpu_vec1)
    print("CPU scalar/vectorized1: %.1f" % (cpu_scalar / cpu_vec1))
    np.random.seed(10)
    pos, pos2, pos_hist, pos_hist_times = random_walks1D_vec2(
        x0, N, p, num_walks, num_times=4
    )
    t3 = time.perf_counter()
    cpu_vec2 = t3 - t2
    print("CPU vectorized2: %.1f" % cpu_vec2)
    print("CPU scalar/vectorized2: %.1f" % (cpu_scalar / cpu_vec2))

def random_walk2D(x0, N, p, random=random):
    """2D random walk with 1 particle and N moves: N, E, W, S."""
    d = len(x0)
    position = np.zeros((N + 1, d))
    position[0, :] = x0
    current_pos = np.array(x0, dtype=float)
    for k in range(N):
        r = random.uniform(0, 1)
        if r <= 0.25:
            current_pos += np.array([0, 1])  # Move north
        elif 0.25 < r <= 0.5:
            current_pos += np.array([1, 0])  # Move east
        elif 0.5 < r <= 0.75:
            current_pos += np.array([0, -1])  # Move south
        else:
            current_pos += np.array([-1, 0])  # Move west
        position[k + 1, :] = current_pos
    return position

def demo_random_walk2D():
    x0 = (0, 0)
    N = 200
    p = 0.5
    np.random.seed(10)
    pos = random_walk2D(x0, N, p, random=np.random)
    plt.plot(pos[:, 0], pos[:, 1])
    plt.savefig("tmp1.png")
    plt.savefig("tmp1.pdf")
    plt.show()

def random_walkdD(x0, N, p, random=random):
    """Any-D (diagonal) random walk with 1 particle and N moves."""
    d = len(x0)
    position = np.zeros((N + 1, d))
    position[0, :] = x0
    current_pos = np.array(x0, dtype=float)
    for k in range(N):
        for i in range(d):
            r = random.uniform(0, 1)
            if r <= p:
                current_pos[i] -= 1
            else:
                current_pos[i] += 1
        position[k + 1, :] = current_pos
    return position

def random_walkdD_vec(x0, N, p):
    """Vectorized version of random_walkdD."""
    d = len(x0)
    position = np.zeros((N + 1, d))
    position[0] = np.array(x0, dtype=float)
    r = np.random.uniform(0, 1, size=N * d)
    steps = np.where(r <= p, -1, 1).reshape(N, d)
    position[1:, :] = x0 + np.cumsum(steps, axis=0)
    return position

def demo_random_walkdD():
    x0 = (0, 0)
    N = 200
    p = 0.5
    np.random.seed(10)
    pos = random_walkdD(x0, N, p, random=np.random)
    plt.plot(pos[:, 0], pos[:, 1])
    plt.savefig("tmp1.png")
    plt.savefig("tmp1.pdf")
    plt.show()

def demo_random_walkdD_timing():
    import time

    x0 = (0, 0)
    N = 4000000
    p = 0.5

    t0 = time.perf_counter()
    np.random.seed(10)
    pos = random_walkdD(x0, N, p, random=np.random)
    t1 = time.perf_counter()
    cpu_scalar = t1 - t0
    print("CPU scalar: %.1f" % cpu_scalar)
    np.random.seed(10)
    pos = random_walkdD_vec(x0, N, p)
    t2 = time.perf_counter()
    cpu_vec = t2 - t1
    print("CPU vectorized: %.1f" % cpu_vec)
    print("CPU scalar/vectorized: %.1f" % (cpu_scalar / cpu_vec))

def demo_fig_random_walkdD():
    x0 = (0, 0)
    N = 5000
    p = 0.5
    n = 2  # nxn subplots
    f, axarr = plt.subplots(n, n, sharex=True, sharey=True)
    for i in range(n):
        for j in range(n):
            seed = 3 * i + 8 * j
            np.random.seed(seed)
            pos = random_walkdD(x0, N, p, random=np.random)
            axarr[i, j].plot(pos[:, 0], pos[:, 1])
    plt.savefig("tmp1.png")
    plt.savefig("tmp1.pdf")
    plt.show()

def test_ramdom_walkdD():
    x0 = (0, 0)
    N = 7
    p = 0.5
    np.random.seed(10)
    scalar_computed = random_walkdD(x0, N, p, random=np.random)
    np.random.seed(10)
    vectorized_computed = random_walkdD_vec(x0, N, p)
    assert (scalar_computed == vectorized_computed).all()

def random_walksdD(x0, N, p, num_walks=1, num_times=1, random=random):
    """Simulate num_walks random walks from x0 with N steps."""
    d = len(x0)
    position = np.zeros((N + 1, d))  # Accumulated positions
    position2 = np.zeros((N + 1, d))  # Accumulated positions**2
    pos_hist = np.zeros((num_walks, num_times, d))
    pos_hist_times = [(N // num_times) * i for i in range(num_times)]

    for n in range(num_walks):
        num_times_counter = 0
        current_pos = np.array(x0, dtype=float)
        for k in range(N):
            if k in pos_hist_times:
                pos_hist[n, num_times_counter, :] = current_pos
                num_times_counter += 1
            for i in range(d):
                r = random.uniform(0, 1)
                if r <= p:
                    current_pos[i] -= 1
                else:
                    current_pos[i] += 1
            position[k + 1, :] += current_pos
            position2[k + 1, :] += current_pos**2
    return position, position2, pos_hist, np.array(pos_hist_times)

def random_walksdD_vec(x0, N, p, num_walks=1, num_times=1):
    """Vectorized version of random_walks1D; no loops."""
    d = len(x0)
    position = np.zeros((N + 1, d))  # Accumulated positions
    position2 = np.zeros((N + 1, d))  # Accumulated positions**2
    walks = np.zeros((num_walks, N + 1, d))  # Positions of each walk
    walks[:, 0, :] = x0
    pos_hist = np.zeros((num_walks, num_times, d))
    pos_hist_times = [(N // num_times) * i for i in range(num_times)]

    r = np.random.uniform(0, 1, size=N * num_walks * d)
    steps = np.where(r <= p, -1, 1).reshape(num_walks, N, d)
    walks[:, 1:, :] = x0 + np.cumsum(steps, axis=1)
    position = np.sum(walks, axis=0)
    position2 = np.sum(walks**2, axis=0)
    pos_hist[:, :, :] = walks[:, pos_hist_times, :]
    return position, position2, pos_hist, np.array(pos_hist_times)

def test_random_walksdD():
    x0 = (0, 0)
    N = 4
    p = 0.5

    num_walks = 1
    np.random.seed(10)
    computed = random_walksdD(x0, N, p, num_walks, random=np.random)
    np.random.seed(10)
    expected = random_walkdD(x0, N, p, random=np.random)
    assert (computed[0] == expected).all()

    np.random.seed(10)
    computed = random_walksdD_vec(x0, N, p, num_walks)
    np.random.seed(10)
    expected = random_walkdD_vec(x0, N, p)
    assert (computed[0] == expected).all()

    num_walks = 3
    num_times = N
    np.random.seed(10)
    serial_computed = random_walksdD(x0, N, p, num_walks, num_times, random=np.random)
    np.random.seed(10)
    vectorized_computed = random_walksdD_vec(x0, N, p, num_walks, num_times)
    return_values = ["pos", "pos2", "pos_hist", "pos_hist_times"]
    for s, v, r in zip(serial_computed, vectorized_computed, return_values):
        msg = "%s: %s\n%s (serial)\nvs\n%s\n%s (vectorized)" % (r, s.shape, s, v.shape, v)
        assert (s == v).all(), msg

def demo_random_walksdD():
    x0 = (0, 0)
    N = 1000
    num_walks = 1000
    p = 0.5
    np.random.seed(10)
    pos, pos2, pos_hist, pos_hist_times = random_walksdD(
        x0, N, p, num_walks, num_times=4, random=np.random
    )
    print(pos_hist_times)
    plt.figure()
    plt.plot(pos[:, 0], pos[:, 1])
    np.random.seed(10)
    pos, pos2, pos_hist, pos_hist_times = random_walksdD_vec(
        x0, N, p, num_walks, num_times=4
    )
    plt.figure()
    plt.plot(pos[:, 0], pos[:, 1])

    plt.show()

def demo_random_walksdD_timing():
    import time

    x0 = (0, 0, 0)
    N = 1000
    num_walks = 10000
    p = 0.5

    t0 = time.perf_counter()
    np.random.seed(10)
    pos, pos2, pos_hist, pos_hist_times = random_walksdD(
        x0, N, p, num_walks, num_times=4, random=np.random
    )
    t1 = time.perf_counter()
    cpu_scalar = t1 - t0
    print("CPU scalar: %.1f" % cpu_scalar)
    np.random.seed(10)
    pos, pos2, pos_hist, pos_hist_times = random_walksdD_vec(
        x0, N, p, num_walks, num_times=4
    )
    t2 = time.perf_counter()
    cpu_vec = t2 - t1
    print("CPU vectorized: %.1f" % cpu_vec)
    print("CPU scalar/vectorized: %.1f" % (cpu_scalar / cpu_vec))

def random_walks1D2(x0, N, p, num_walks=1, num_times=1, random=random):
    """Simulate num_walks random walks from x0 with N steps."""
    position = np.zeros(N + 1)  # Accumulated positions
    position[0] = x0 * num_walks
    position2 = np.zeros(N + 1)  # Accumulated positions**2
    position2[0] = x0**2 * num_walks
    pos_hist = np.zeros((num_walks, num_times))
    pos_hist_times = [(N // num_times) * i for i in range(num_times)]

    current_pos = x0 + np.zeros(num_walks)
    num_times_counter = -1

    for k in range(N):
        if k in pos_hist_times:
            num_times_counter += 1
            store_hist = True  # Store histogram data for this k
        else:
            store_hist = False

        for n in range(num_walks):
            r = random.uniform(0, 1)
            if r <= p:
                current_pos[n] -= 1
            else:
                current_pos[n] += 1
            position[k + 1] += current_pos[n]
            position2[k + 1] += current_pos[n] ** 2
            if store_hist:
                pos_hist[n, num_times_counter] = current_pos[n]
    return position, position2, pos_hist, np.array(pos_hist_times)

def random_walks1D2_vec1(x0, N, p, num_walks=1, num_times=1):
    """Vectorized version of random_walks1D2."""
    position = np.zeros(N + 1)  # Accumulated positions
    position2 = np.zeros(N + 1)  # Accumulated positions**2
    pos_hist = np.zeros((num_walks, num_times))
    pos_hist_times = [(N // num_times) * i for i in range(num_times)]

    current_pos = np.zeros(num_walks)
    current_pos[0] = x0
    num_times_counter = -1

    for k in range(N):
        if k in pos_hist_times:
            num_times_counter += 1
            store_hist = True  # Store histogram data for this k
        else:
            store_hist = False

        r = np.random.uniform(0, 1, size=num_walks)
        steps = np.where(r <= p, -1, 1)
        current_pos += steps
        position[k + 1] = np.sum(current_pos)
        position2[k + 1] = np.sum(current_pos**2)
        if store_hist:
            pos_hist[:, num_times_counter] = current_pos
    return position, position2, pos_hist, np.array(pos_hist_times)

def test_random_walks1D2():
    x0 = 0
    N = 4
    p = 0.5
    num_walks = 3
    num_times = N
    np.random.seed(10)
    serial_computed = random_walks1D2(x0, N, p, num_walks, num_times, random=np.random)
    np.random.seed(10)
    vectorized_computed = random_walks1D2_vec1(x0, N, p, num_walks, num_times)
    return_values = ["pos", "pos2", "pos_hist", "pos_hist_times"]
    for s, v, r in zip(serial_computed, vectorized_computed, return_values):
        msg = "%s: %s (serial) vs %s (vectorized)" % (r, s, v)
        assert (s == v).all(), msg

def demo_random_walks1D2_timing():
    """Timing of random 1D walks with reversed loops."""
    import time

    x0 = 0
    N = 1000
    num_walks = 50000
    p = 0.5

    t0 = time.perf_counter()
    np.random.seed(10)
    pos, pos2, pos_hist, pos_hist_times = random_walks1D2(
        x0, N, p, num_walks, num_times=4, random=np.random
    )
    t1 = time.perf_counter()
    cpu_scalar = t1 - t0
    print("CPU scalar: %.1f" % cpu_scalar)
    np.random.seed(10)
    pos, pos2, pos_hist, pos_hist_times = random_walks1D2_vec1(
        x0, N, p, num_walks, num_times=4
    )
    t2 = time.perf_counter()
    cpu_vec1 = t2 - t1
    print("CPU vectorized1: %.1f" % cpu_vec1)
    print("CPU scalar/vectorized1: %.1f" % (cpu_scalar / cpu_vec1))
    np.random.seed(10)
    pos, pos2, pos_hist, pos_hist_times = random_walks1D_vec2(
        x0, N, p, num_walks, num_times=4
    )
    t3 = time.perf_counter()
    cpu_vec2 = t3 - t2
    print("CPU vectorized2: %.1f" % cpu_vec2)
    print("CPU scalar/vectorized2: %.1f" % (cpu_scalar / cpu_vec2))

if __name__ == "__main__":
    demo_random_walks1D2_timing()
    print("----")
    demo_random_walks1D_timing()
```

A vectorized version is desired. We follow the ideas from Section
@sec-diffu-randomwalk-1D-code1, but each step is now a vector in $d$
spatial dimensions. We therefore need to draw $And$ random numbers in `r`,
compute steps in the various directions through `np.where(r <=p, -1, 1)`
(each step being $-1$ or $1$),
and then we can reshape this array to an $N\times d$ array of step
*vectors*. Doing an `np.cumsum` summation along axis 0 will add
the vectors, as this demo shows:

```python
>>> a = np.arrange(6).reshape(3,2)
>>> a
array([[0, 1],
       [2, 3],
       [4, 5]])
>>> np.cumsum(a, axis=0)
array([[ 0,  1],
       [ 2,  4],
       [ 6,  9]])
```
With such summation of step vectors, we get all the positions to be
filled in the `position` array:

```python
import random

import matplotlib.pyplot as plt
import numpy as np

random.seed(10)
np.random.seed(10)

def random_walk1D(x0, N, p, random=random):
    """1D random walk with 1 particle and N moves."""

    position = np.zeros(N + 1)
    position[0] = x0
    current_pos = x0
    for k in range(N):
        r = random.uniform(0, 1)
        if r <= p:
            current_pos -= 1
        else:
            current_pos += 1
        position[k + 1] = current_pos
    return position

def random_walk1D_vec(x0, N, p):
    """Vectorized version of random_walk1D."""
    position = np.zeros(N + 1)
    position[0] = x0
    r = np.random.uniform(0, 1, size=N)
    steps = np.where(r <= p, -1, 1)
    position[1:] = x0 + np.cumsum(steps)
    return position

def test_random_walk1D():
    x0 = 2
    N = 4
    p = 0.6
    np.random.seed(10)
    scalar_computed = random_walk1D(x0, N, p, random=np.random)
    np.random.seed(10)
    vectorized_computed = random_walk1D_vec(x0, N, p)
    assert (scalar_computed == vectorized_computed).all()

def demo_random_walk1D(N=50000):
    np.random.seed(10)
    pos = random_walk1D_vec(x0=0, N=N, p=0.5)
    plt.figure()
    plt.plot(pos)
    plt.savefig("tmp1.pdf")
    plt.savefig("tmp1.png")
    plt.figure()
    plt.plot(pos * pos)
    plt.savefig("tmp2.pdf")
    plt.savefig("tmp2.png")
    plt.show()

def demo_fig_random_walk1D(N=200):
    """Make ensamble of positions (to illustrate E[] operator)."""
    np.random.seed(10)
    num_plots = 4
    for n in range(num_plots):
        plt.subplot(num_plots, 1, n + 1)
        pos = random_walk1D_vec(x0=0, N=N, p=0.5)
        plt.plot(pos)
        plt.axis([0, N, -15, 20])
    plt.savefig("tmp.pdf")
    plt.savefig("tmp.png")
    plt.show()

def demo_random_walk1D_timing():
    import time

    x0 = 0
    N = 10000000
    p = 0.5

    t0 = time.perf_counter()
    np.random.seed(10)
    pos = random_walk1D(x0, N, p, random=np.random)
    t1 = time.perf_counter()
    cpu_scalar = t1 - t0
    print("CPU scalar: %.1f" % cpu_scalar)
    np.random.seed(10)
    pos = random_walk1D_vec(x0, N, p)
    t2 = time.perf_counter()
    cpu_vec = t2 - t1
    print("CPU vectorized: %.1f" % cpu_vec)
    print("CPU scalar/vectorized: %.1f" % (cpu_scalar / cpu_vec))

def random_walks1D(x0, N, p, num_walks=1, num_times=1, random=random):
    """Simulate num_walks random walks from x0 with N steps."""
    position = np.zeros(N + 1)  # Accumulated positions
    position[0] = x0 * num_walks
    position2 = np.zeros(N + 1)  # Accumulated positions**2
    position2[0] = x0**2 * num_walks
    pos_hist = np.zeros((num_walks, num_times))
    pos_hist_times = [(N // num_times) * i for i in range(num_times)]

    for n in range(num_walks):
        num_times_counter = 0
        current_pos = x0
        for k in range(N):
            if k in pos_hist_times:
                pos_hist[n, num_times_counter] = current_pos
                num_times_counter += 1
            r = random.uniform(0, 1)
            if r <= p:
                current_pos -= 1
            else:
                current_pos += 1
            position[k + 1] += current_pos
            position2[k + 1] += current_pos**2
    return position, position2, pos_hist, np.array(pos_hist_times)

def random_walks1D_vec1(x0, N, p, num_walks=1, num_times=1):
    """Vectorized version of random_walks1D."""
    position = np.zeros(N + 1)  # Accumulated positions
    position2 = np.zeros(N + 1)  # Accumulated positions**2
    walk = np.zeros(N + 1)  # Positions of current walk
    walk[0] = x0
    pos_hist = np.zeros((num_walks, num_times))
    pos_hist_times = [(N // num_times) * i for i in range(num_times)]

    for n in range(num_walks):
        r = np.random.uniform(0, 1, size=N)
        steps = np.where(r <= p, -1, 1)
        walk[1:] = x0 + np.cumsum(steps)  # Positions of this walk
        position += walk
        position2 += walk**2
        pos_hist[n, :] = walk[pos_hist_times]
    return position, position2, pos_hist, np.array(pos_hist_times)

def random_walks1D_vec2(x0, N, p, num_walks=1, num_times=1):
    """Vectorized version of random_walks1D; no loops."""
    position = np.zeros(N + 1)  # Accumulated positions
    position2 = np.zeros(N + 1)  # Accumulated positions**2
    walks = np.zeros((num_walks, N + 1))  # Positions of each walk
    walks[:, 0] = x0
    pos_hist = np.zeros((num_walks, num_times))
    pos_hist_times = [(N // num_times) * i for i in range(num_times)]

    r = np.random.uniform(0, 1, size=N * num_walks)
    steps = np.where(r <= p, -1, 1).reshape(num_walks, N)
    walks[:, 1:] = x0 + np.cumsum(steps, axis=1)
    position = np.sum(walks, axis=0)
    position2 = np.sum(walks**2, axis=0)
    pos_hist[:, :] = walks[:, pos_hist_times]
    return position, position2, pos_hist, np.array(pos_hist_times)

def test_random_walks1D():
    x0 = 0
    N = 4
    p = 0.5

    num_walks = 1
    np.random.seed(10)
    computed = random_walks1D(x0, N, p, num_walks, random=np.random)
    np.random.seed(10)
    expected = random_walk1D(x0, N, p, random=np.random)
    assert (computed[0] == expected).all()

    np.random.seed(10)
    computed = random_walks1D_vec1(x0, N, p, num_walks)
    np.random.seed(10)
    expected = random_walk1D_vec(x0, N, p)
    assert (computed[0] == expected).all()
    np.random.seed(10)
    computed = random_walks1D_vec2(x0, N, p, num_walks)
    np.random.seed(10)
    expected = random_walk1D_vec(x0, N, p)
    assert (computed[0] == expected).all()

    num_walks = 3
    num_times = N
    np.random.seed(10)
    serial_computed = random_walks1D(x0, N, p, num_walks, num_times, random=np.random)
    np.random.seed(10)
    vectorized1_computed = random_walks1D_vec1(x0, N, p, num_walks, num_times)
    np.random.seed(10)
    vectorized2_computed = random_walks1D_vec2(x0, N, p, num_walks, num_times)
    return_values = ["pos", "pos2", "pos_hist", "pos_hist_times"]
    for s, v, r in zip(serial_computed, vectorized1_computed, return_values):
        msg = "%s: %s (serial) vs %s (vectorized)" % (r, s, v)
        assert (s == v).all(), msg
    for s, v, r in zip(serial_computed, vectorized2_computed, return_values):
        msg = "%s: %s (serial) vs %s (vectorized)" % (r, s, v)
        assert (s == v).all(), msg

def demo_random_walks1D(N=1000, num_walks=10000, EX_minmax=None):
    import time

    t0 = time.perf_counter()
    pos, pos2, hist, hist_times = random_walks1D_vec1(
        x0=0,
        N=N,
        p=0.5,
        num_walks=num_walks,
        num_times=10,
    )
    t1 = time.perf_counter()
    print("histogram times:", hist_times)
    print("random walk: %.1fs" % (t1 - t0))
    E_X = pos / float(num_walks)
    Var_X = pos2 / float(num_walks) - E_X**2
    if N <= 50:
        print(pos)

    plt.figure()
    plt.plot(E_X)
    if EX_minmax is not None:
        plt.axis([0, N, EX_minmax[0], EX_minmax[1]])
    plt.title("Expected position (%d walks)" % num_walks)
    plt.savefig("tmp1.png")
    plt.savefig("tmp1.pdf")
    plt.figure()
    plt.plot(Var_X)
    plt.title("Variance of position (%d walks)" % num_walks)
    plt.savefig("tmp2.png")
    plt.savefig("tmp2.pdf")

    plt.figure()
    a = 0.5
    exact = (
        lambda x, t: 1.0 / np.sqrt(4 * np.pi * t * a) * np.exp(-(x**2) / (4.0 * t * a))
    )
    hist_time_index = -2
    n, bins, patches = plt.hist(hist[:, hist_time_index], bins=30, normed=True)
    x = np.linspace(bins[0], bins[-1], 301)
    t = hist_times[hist_time_index]
    plt.plot(x, exact(x, t), "r--")
    plt.title("Histogram of positions (%d walks)" % num_walks)
    plt.savefig("tmp3.png")
    plt.savefig("tmp3.pdf")
    plt.show()

def demo_fig_random_walks1D():
    """Make figures with statistics and dependence on no of walks."""
    import os
    import shutil

    N = 1000
    num_walks = [100, 10000, 100000, 1000000]
    for n in num_walks:
        np.random.seed(10)  # Use same seq. for all experiments
        if n == 100:
            demo_random_walks1D(N=N, num_walks=n, EX_minmax=None)
        else:
            demo_random_walks1D(N=N, num_walks=n, EX_minmax=[-0.1, 0.5])
        d = "tmp_%d" % n
        if os.path.isdir(d):
            shutil.rmtree(d)
        os.mkdir(d)
        for p in 1, 2, 3:
            os.rename("tmp%d.png" % p, os.path.join(d, "tmp%d.png" % p))
            os.rename("tmp%d.pdf" % p, os.path.join(d, "tmp%d.pdf" % p))
    plots = ["EX", "VarX", "HistX"]
    for j, plot in enumerate(plots):
        for ext in "png", "pdf":
            files = [
                os.path.join("tmp_%d" % n, "tmp%d.%s" % (j + 1, ext)) for n in num_walks
            ]
            cmd = "doconce combine_images -%d %s rw1D_%s_%s.%s" % (
                3 if len(num_walks) == 3 else 2,
                " ".join(files),
                plot,
                "_".join([str(n) for n in num_walks]),
                ext,
            )
            print(cmd)
            os.system(cmd)

def demo_random_walks1D_timing():
    import time

    x0 = 0
    N = 1000
    num_walks = 50000
    p = 0.5

    t0 = time.perf_counter()
    np.random.seed(10)
    pos, pos2, pos_hist, pos_hist_times = random_walks1D(
        x0, N, p, num_walks, num_times=4, random=np.random
    )
    t1 = time.perf_counter()
    cpu_scalar = t1 - t0
    print("CPU scalar: %.1f" % cpu_scalar)
    np.random.seed(10)
    pos, pos2, pos_hist, pos_hist_times = random_walks1D_vec1(
        x0, N, p, num_walks, num_times=4
    )
    t2 = time.perf_counter()
    cpu_vec1 = t2 - t1
    print("CPU vectorized1: %.1f" % cpu_vec1)
    print("CPU scalar/vectorized1: %.1f" % (cpu_scalar / cpu_vec1))
    np.random.seed(10)
    pos, pos2, pos_hist, pos_hist_times = random_walks1D_vec2(
        x0, N, p, num_walks, num_times=4
    )
    t3 = time.perf_counter()
    cpu_vec2 = t3 - t2
    print("CPU vectorized2: %.1f" % cpu_vec2)
    print("CPU scalar/vectorized2: %.1f" % (cpu_scalar / cpu_vec2))

def random_walk2D(x0, N, p, random=random):
    """2D random walk with 1 particle and N moves: N, E, W, S."""
    d = len(x0)
    position = np.zeros((N + 1, d))
    position[0, :] = x0
    current_pos = np.array(x0, dtype=float)
    for k in range(N):
        r = random.uniform(0, 1)
        if r <= 0.25:
            current_pos += np.array([0, 1])  # Move north
        elif 0.25 < r <= 0.5:
            current_pos += np.array([1, 0])  # Move east
        elif 0.5 < r <= 0.75:
            current_pos += np.array([0, -1])  # Move south
        else:
            current_pos += np.array([-1, 0])  # Move west
        position[k + 1, :] = current_pos
    return position

def demo_random_walk2D():
    x0 = (0, 0)
    N = 200
    p = 0.5
    np.random.seed(10)
    pos = random_walk2D(x0, N, p, random=np.random)
    plt.plot(pos[:, 0], pos[:, 1])
    plt.savefig("tmp1.png")
    plt.savefig("tmp1.pdf")
    plt.show()

def random_walkdD(x0, N, p, random=random):
    """Any-D (diagonal) random walk with 1 particle and N moves."""
    d = len(x0)
    position = np.zeros((N + 1, d))
    position[0, :] = x0
    current_pos = np.array(x0, dtype=float)
    for k in range(N):
        for i in range(d):
            r = random.uniform(0, 1)
            if r <= p:
                current_pos[i] -= 1
            else:
                current_pos[i] += 1
        position[k + 1, :] = current_pos
    return position

def random_walkdD_vec(x0, N, p):
    """Vectorized version of random_walkdD."""
    d = len(x0)
    position = np.zeros((N + 1, d))
    position[0] = np.array(x0, dtype=float)
    r = np.random.uniform(0, 1, size=N * d)
    steps = np.where(r <= p, -1, 1).reshape(N, d)
    position[1:, :] = x0 + np.cumsum(steps, axis=0)
    return position
```

![Four random walks with 5000 steps in 2D.](fig/rw2D_samples_5000){#fig-diffu-randomwalk-2D-fig-samples width="100%"}

## Multiple random walks in any number of space dimensions
As we did in 1D, we extend one single walk to a number of walks (`num_walks`
in the code).

### Scalar code
As always, we start with implementing the scalar case:

```python
import random

import matplotlib.pyplot as plt
import numpy as np

random.seed(10)
np.random.seed(10)

def random_walk1D(x0, N, p, random=random):
    """1D random walk with 1 particle and N moves."""

    position = np.zeros(N + 1)
    position[0] = x0
    current_pos = x0
    for k in range(N):
        r = random.uniform(0, 1)
        if r <= p:
            current_pos -= 1
        else:
            current_pos += 1
        position[k + 1] = current_pos
    return position

def random_walk1D_vec(x0, N, p):
    """Vectorized version of random_walk1D."""
    position = np.zeros(N + 1)
    position[0] = x0
    r = np.random.uniform(0, 1, size=N)
    steps = np.where(r <= p, -1, 1)
    position[1:] = x0 + np.cumsum(steps)
    return position

def test_random_walk1D():
    x0 = 2
    N = 4
    p = 0.6
    np.random.seed(10)
    scalar_computed = random_walk1D(x0, N, p, random=np.random)
    np.random.seed(10)
    vectorized_computed = random_walk1D_vec(x0, N, p)
    assert (scalar_computed == vectorized_computed).all()

def demo_random_walk1D(N=50000):
    np.random.seed(10)
    pos = random_walk1D_vec(x0=0, N=N, p=0.5)
    plt.figure()
    plt.plot(pos)
    plt.savefig("tmp1.pdf")
    plt.savefig("tmp1.png")
    plt.figure()
    plt.plot(pos * pos)
    plt.savefig("tmp2.pdf")
    plt.savefig("tmp2.png")
    plt.show()

def demo_fig_random_walk1D(N=200):
    """Make ensamble of positions (to illustrate E[] operator)."""
    np.random.seed(10)
    num_plots = 4
    for n in range(num_plots):
        plt.subplot(num_plots, 1, n + 1)
        pos = random_walk1D_vec(x0=0, N=N, p=0.5)
        plt.plot(pos)
        plt.axis([0, N, -15, 20])
    plt.savefig("tmp.pdf")
    plt.savefig("tmp.png")
    plt.show()

def demo_random_walk1D_timing():
    import time

    x0 = 0
    N = 10000000
    p = 0.5

    t0 = time.perf_counter()
    np.random.seed(10)
    pos = random_walk1D(x0, N, p, random=np.random)
    t1 = time.perf_counter()
    cpu_scalar = t1 - t0
    print("CPU scalar: %.1f" % cpu_scalar)
    np.random.seed(10)
    pos = random_walk1D_vec(x0, N, p)
    t2 = time.perf_counter()
    cpu_vec = t2 - t1
    print("CPU vectorized: %.1f" % cpu_vec)
    print("CPU scalar/vectorized: %.1f" % (cpu_scalar / cpu_vec))

def random_walks1D(x0, N, p, num_walks=1, num_times=1, random=random):
    """Simulate num_walks random walks from x0 with N steps."""
    position = np.zeros(N + 1)  # Accumulated positions
    position[0] = x0 * num_walks
    position2 = np.zeros(N + 1)  # Accumulated positions**2
    position2[0] = x0**2 * num_walks
    pos_hist = np.zeros((num_walks, num_times))
    pos_hist_times = [(N // num_times) * i for i in range(num_times)]

    for n in range(num_walks):
        num_times_counter = 0
        current_pos = x0
        for k in range(N):
            if k in pos_hist_times:
                pos_hist[n, num_times_counter] = current_pos
                num_times_counter += 1
            r = random.uniform(0, 1)
            if r <= p:
                current_pos -= 1
            else:
                current_pos += 1
            position[k + 1] += current_pos
            position2[k + 1] += current_pos**2
    return position, position2, pos_hist, np.array(pos_hist_times)

def random_walks1D_vec1(x0, N, p, num_walks=1, num_times=1):
    """Vectorized version of random_walks1D."""
    position = np.zeros(N + 1)  # Accumulated positions
    position2 = np.zeros(N + 1)  # Accumulated positions**2
    walk = np.zeros(N + 1)  # Positions of current walk
    walk[0] = x0
    pos_hist = np.zeros((num_walks, num_times))
    pos_hist_times = [(N // num_times) * i for i in range(num_times)]

    for n in range(num_walks):
        r = np.random.uniform(0, 1, size=N)
        steps = np.where(r <= p, -1, 1)
        walk[1:] = x0 + np.cumsum(steps)  # Positions of this walk
        position += walk
        position2 += walk**2
        pos_hist[n, :] = walk[pos_hist_times]
    return position, position2, pos_hist, np.array(pos_hist_times)

def random_walks1D_vec2(x0, N, p, num_walks=1, num_times=1):
    """Vectorized version of random_walks1D; no loops."""
    position = np.zeros(N + 1)  # Accumulated positions
    position2 = np.zeros(N + 1)  # Accumulated positions**2
    walks = np.zeros((num_walks, N + 1))  # Positions of each walk
    walks[:, 0] = x0
    pos_hist = np.zeros((num_walks, num_times))
    pos_hist_times = [(N // num_times) * i for i in range(num_times)]

    r = np.random.uniform(0, 1, size=N * num_walks)
    steps = np.where(r <= p, -1, 1).reshape(num_walks, N)
    walks[:, 1:] = x0 + np.cumsum(steps, axis=1)
    position = np.sum(walks, axis=0)
    position2 = np.sum(walks**2, axis=0)
    pos_hist[:, :] = walks[:, pos_hist_times]
    return position, position2, pos_hist, np.array(pos_hist_times)

def test_random_walks1D():
    x0 = 0
    N = 4
    p = 0.5

    num_walks = 1
    np.random.seed(10)
    computed = random_walks1D(x0, N, p, num_walks, random=np.random)
    np.random.seed(10)
    expected = random_walk1D(x0, N, p, random=np.random)
    assert (computed[0] == expected).all()

    np.random.seed(10)
    computed = random_walks1D_vec1(x0, N, p, num_walks)
    np.random.seed(10)
    expected = random_walk1D_vec(x0, N, p)
    assert (computed[0] == expected).all()
    np.random.seed(10)
    computed = random_walks1D_vec2(x0, N, p, num_walks)
    np.random.seed(10)
    expected = random_walk1D_vec(x0, N, p)
    assert (computed[0] == expected).all()

    num_walks = 3
    num_times = N
    np.random.seed(10)
    serial_computed = random_walks1D(x0, N, p, num_walks, num_times, random=np.random)
    np.random.seed(10)
    vectorized1_computed = random_walks1D_vec1(x0, N, p, num_walks, num_times)
    np.random.seed(10)
    vectorized2_computed = random_walks1D_vec2(x0, N, p, num_walks, num_times)
    return_values = ["pos", "pos2", "pos_hist", "pos_hist_times"]
    for s, v, r in zip(serial_computed, vectorized1_computed, return_values):
        msg = "%s: %s (serial) vs %s (vectorized)" % (r, s, v)
        assert (s == v).all(), msg
    for s, v, r in zip(serial_computed, vectorized2_computed, return_values):
        msg = "%s: %s (serial) vs %s (vectorized)" % (r, s, v)
        assert (s == v).all(), msg

def demo_random_walks1D(N=1000, num_walks=10000, EX_minmax=None):
    import time

    t0 = time.perf_counter()
    pos, pos2, hist, hist_times = random_walks1D_vec1(
        x0=0,
        N=N,
        p=0.5,
        num_walks=num_walks,
        num_times=10,
    )
    t1 = time.perf_counter()
    print("histogram times:", hist_times)
    print("random walk: %.1fs" % (t1 - t0))
    E_X = pos / float(num_walks)
    Var_X = pos2 / float(num_walks) - E_X**2
    if N <= 50:
        print(pos)

    plt.figure()
    plt.plot(E_X)
    if EX_minmax is not None:
        plt.axis([0, N, EX_minmax[0], EX_minmax[1]])
    plt.title("Expected position (%d walks)" % num_walks)
    plt.savefig("tmp1.png")
    plt.savefig("tmp1.pdf")
    plt.figure()
    plt.plot(Var_X)
    plt.title("Variance of position (%d walks)" % num_walks)
    plt.savefig("tmp2.png")
    plt.savefig("tmp2.pdf")

    plt.figure()
    a = 0.5
    exact = (
        lambda x, t: 1.0 / np.sqrt(4 * np.pi * t * a) * np.exp(-(x**2) / (4.0 * t * a))
    )
    hist_time_index = -2
    n, bins, patches = plt.hist(hist[:, hist_time_index], bins=30, normed=True)
    x = np.linspace(bins[0], bins[-1], 301)
    t = hist_times[hist_time_index]
    plt.plot(x, exact(x, t), "r--")
    plt.title("Histogram of positions (%d walks)" % num_walks)
    plt.savefig("tmp3.png")
    plt.savefig("tmp3.pdf")
    plt.show()

def demo_fig_random_walks1D():
    """Make figures with statistics and dependence on no of walks."""
    import os
    import shutil

    N = 1000
    num_walks = [100, 10000, 100000, 1000000]
    for n in num_walks:
        np.random.seed(10)  # Use same seq. for all experiments
        if n == 100:
            demo_random_walks1D(N=N, num_walks=n, EX_minmax=None)
        else:
            demo_random_walks1D(N=N, num_walks=n, EX_minmax=[-0.1, 0.5])
        d = "tmp_%d" % n
        if os.path.isdir(d):
            shutil.rmtree(d)
        os.mkdir(d)
        for p in 1, 2, 3:
            os.rename("tmp%d.png" % p, os.path.join(d, "tmp%d.png" % p))
            os.rename("tmp%d.pdf" % p, os.path.join(d, "tmp%d.pdf" % p))
    plots = ["EX", "VarX", "HistX"]
    for j, plot in enumerate(plots):
        for ext in "png", "pdf":
            files = [
                os.path.join("tmp_%d" % n, "tmp%d.%s" % (j + 1, ext)) for n in num_walks
            ]
            cmd = "doconce combine_images -%d %s rw1D_%s_%s.%s" % (
                3 if len(num_walks) == 3 else 2,
                " ".join(files),
                plot,
                "_".join([str(n) for n in num_walks]),
                ext,
            )
            print(cmd)
            os.system(cmd)

def demo_random_walks1D_timing():
    import time

    x0 = 0
    N = 1000
    num_walks = 50000
    p = 0.5

    t0 = time.perf_counter()
    np.random.seed(10)
    pos, pos2, pos_hist, pos_hist_times = random_walks1D(
        x0, N, p, num_walks, num_times=4, random=np.random
    )
    t1 = time.perf_counter()
    cpu_scalar = t1 - t0
    print("CPU scalar: %.1f" % cpu_scalar)
    np.random.seed(10)
    pos, pos2, pos_hist, pos_hist_times = random_walks1D_vec1(
        x0, N, p, num_walks, num_times=4
    )
    t2 = time.perf_counter()
    cpu_vec1 = t2 - t1
    print("CPU vectorized1: %.1f" % cpu_vec1)
    print("CPU scalar/vectorized1: %.1f" % (cpu_scalar / cpu_vec1))
    np.random.seed(10)
    pos, pos2, pos_hist, pos_hist_times = random_walks1D_vec2(
        x0, N, p, num_walks, num_times=4
    )
    t3 = time.perf_counter()
    cpu_vec2 = t3 - t2
    print("CPU vectorized2: %.1f" % cpu_vec2)
    print("CPU scalar/vectorized2: %.1f" % (cpu_scalar / cpu_vec2))

def random_walk2D(x0, N, p, random=random):
    """2D random walk with 1 particle and N moves: N, E, W, S."""
    d = len(x0)
    position = np.zeros((N + 1, d))
    position[0, :] = x0
    current_pos = np.array(x0, dtype=float)
    for k in range(N):
        r = random.uniform(0, 1)
        if r <= 0.25:
            current_pos += np.array([0, 1])  # Move north
        elif 0.25 < r <= 0.5:
            current_pos += np.array([1, 0])  # Move east
        elif 0.5 < r <= 0.75:
            current_pos += np.array([0, -1])  # Move south
        else:
            current_pos += np.array([-1, 0])  # Move west
        position[k + 1, :] = current_pos
    return position

def demo_random_walk2D():
    x0 = (0, 0)
    N = 200
    p = 0.5
    np.random.seed(10)
    pos = random_walk2D(x0, N, p, random=np.random)
    plt.plot(pos[:, 0], pos[:, 1])
    plt.savefig("tmp1.png")
    plt.savefig("tmp1.pdf")
    plt.show()

def random_walkdD(x0, N, p, random=random):
    """Any-D (diagonal) random walk with 1 particle and N moves."""
    d = len(x0)
    position = np.zeros((N + 1, d))
    position[0, :] = x0
    current_pos = np.array(x0, dtype=float)
    for k in range(N):
        for i in range(d):
            r = random.uniform(0, 1)
            if r <= p:
                current_pos[i] -= 1
            else:
                current_pos[i] += 1
        position[k + 1, :] = current_pos
    return position

def random_walkdD_vec(x0, N, p):
    """Vectorized version of random_walkdD."""
    d = len(x0)
    position = np.zeros((N + 1, d))
    position[0] = np.array(x0, dtype=float)
    r = np.random.uniform(0, 1, size=N * d)
    steps = np.where(r <= p, -1, 1).reshape(N, d)
    position[1:, :] = x0 + np.cumsum(steps, axis=0)
    return position

def demo_random_walkdD():
    x0 = (0, 0)
    N = 200
    p = 0.5
    np.random.seed(10)
    pos = random_walkdD(x0, N, p, random=np.random)
    plt.plot(pos[:, 0], pos[:, 1])
    plt.savefig("tmp1.png")
    plt.savefig("tmp1.pdf")
    plt.show()

def demo_random_walkdD_timing():
    import time

    x0 = (0, 0)
    N = 4000000
    p = 0.5

    t0 = time.perf_counter()
    np.random.seed(10)
    pos = random_walkdD(x0, N, p, random=np.random)
    t1 = time.perf_counter()
    cpu_scalar = t1 - t0
    print("CPU scalar: %.1f" % cpu_scalar)
    np.random.seed(10)
    pos = random_walkdD_vec(x0, N, p)
    t2 = time.perf_counter()
    cpu_vec = t2 - t1
    print("CPU vectorized: %.1f" % cpu_vec)
    print("CPU scalar/vectorized: %.1f" % (cpu_scalar / cpu_vec))

def demo_fig_random_walkdD():
    x0 = (0, 0)
    N = 5000
    p = 0.5
    n = 2  # nxn subplots
    f, axarr = plt.subplots(n, n, sharex=True, sharey=True)
    for i in range(n):
        for j in range(n):
            seed = 3 * i + 8 * j
            np.random.seed(seed)
            pos = random_walkdD(x0, N, p, random=np.random)
            axarr[i, j].plot(pos[:, 0], pos[:, 1])
    plt.savefig("tmp1.png")
    plt.savefig("tmp1.pdf")
    plt.show()

def test_ramdom_walkdD():
    x0 = (0, 0)
    N = 7
    p = 0.5
    np.random.seed(10)
    scalar_computed = random_walkdD(x0, N, p, random=np.random)
    np.random.seed(10)
    vectorized_computed = random_walkdD_vec(x0, N, p)
    assert (scalar_computed == vectorized_computed).all()

def random_walksdD(x0, N, p, num_walks=1, num_times=1, random=random):
    """Simulate num_walks random walks from x0 with N steps."""
    d = len(x0)
    position = np.zeros((N + 1, d))  # Accumulated positions
    position2 = np.zeros((N + 1, d))  # Accumulated positions**2
    pos_hist = np.zeros((num_walks, num_times, d))
    pos_hist_times = [(N // num_times) * i for i in range(num_times)]

    for n in range(num_walks):
        num_times_counter = 0
        current_pos = np.array(x0, dtype=float)
        for k in range(N):
            if k in pos_hist_times:
                pos_hist[n, num_times_counter, :] = current_pos
                num_times_counter += 1
            for i in range(d):
                r = random.uniform(0, 1)
                if r <= p:
                    current_pos[i] -= 1
                else:
                    current_pos[i] += 1
            position[k + 1, :] += current_pos
            position2[k + 1, :] += current_pos**2
    return position, position2, pos_hist, np.array(pos_hist_times)
```

### Vectorized code
Significant speed-ups can be obtained by vectorization. We get rid of
the loops in the previous function and arrive at the following vectorized
code.

```python
def random_walksdD_vec(x0, N, p, num_walks=1, num_times=1):
    """Vectorized version of random_walks1D; no loops."""
    d = len(x0)
    position = np.zeros((N + 1, d))  # Accumulated positions
    position2 = np.zeros((N + 1, d))  # Accumulated positions**2
    walks = np.zeros((num_walks, N + 1, d))  # Positions of each walk
    walks[:, 0, :] = x0
    pos_hist = np.zeros((num_walks, num_times, d))
    pos_hist_times = [(N // num_times) * i for i in range(num_times)]

    r = np.random.uniform(0, 1, size=N * num_walks * d)
    steps = np.where(r <= p, -1, 1).reshape(num_walks, N, d)
    walks[:, 1:, :] = x0 + np.cumsum(steps, axis=1)
    position = np.sum(walks, axis=0)
    position2 = np.sum(walks**2, axis=0)
    pos_hist[:, :, :] = walks[:, pos_hist_times, :]
    return position, position2, pos_hist, np.array(pos_hist_times)
```
